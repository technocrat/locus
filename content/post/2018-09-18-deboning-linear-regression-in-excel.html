---
title: Deboning linear regression output in Excel
author: Richard Careaga
date: '2018-09-18'
slug: deboning-linear-regression-in-excel
categories:
  - Data Science
tags:
  - Excel
  - statistics
  - R
---



<p>A while back (<a href="https://goo.gl/1W11Zu" class="uri">https://goo.gl/1W11Zu</a>), I outlined interpretation of the output of a multiple linear regression of data on Seattle area housing prices (<a href="https://www.kaggle.com/harlfoxem/housesalesprediction?login=true" class="uri">https://www.kaggle.com/harlfoxem/housesalesprediction?login=true</a>), which provides a convenient way to illustrate the usual output of a multiple linear regression model output in <strong>R</strong>. This is a 21K dataset with 19 variables on housing characteristics and sales price. It’s a cruddy model, used solely to pick apart the different data presented. Today, it’s <strong>Excel’s</strong> turn.</p>
<p>Disclosure: I’m not a fan of GUI for most applications. I find it slower and more error prone. Doing this replication elicited the usual grumbles, along with annoyance that multiple independent variables had to be in adjoining columns. Not hard to do, but … .</p>
<p>For comparison, here is the output from the R example, with the addition of an analysis of variance (ANOVA) table that Excel provides by default, but R doesn’t.</p>
<pre class="r"><code>library(readr)
library(knitr)
library(kableExtra)
print(&quot;Data Structure&quot;)</code></pre>
<pre><code>## [1] &quot;Data Structure&quot;</code></pre>
<pre class="r"><code>house &lt;- read_csv(&#39;/Users/rc/projects/statistics-for-data-scientists/kc_house_data.csv&#39;, comment = &#39;&#39;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   id = col_character(),
##   date = col_datetime(format = &quot;&quot;)
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<pre><code>## Warning: 1 parsing failure.
##  row col   expected     actual                                                                  file
## 3165  -- 21 columns 19 columns &#39;/Users/rc/projects/statistics-for-data-scientists/kc_house_data.csv&#39;</code></pre>
<pre class="r"><code>spec(house)</code></pre>
<pre><code>## cols(
##   id = col_character(),
##   date = col_datetime(format = &quot;&quot;),
##   price = col_double(),
##   bedrooms = col_double(),
##   bathrooms = col_double(),
##   sqft_living = col_double(),
##   sqft_lot = col_double(),
##   floors = col_double(),
##   waterfront = col_double(),
##   view = col_double(),
##   condition = col_double(),
##   grade = col_double(),
##   sqft_above = col_double(),
##   sqft_basement = col_double(),
##   yr_built = col_double(),
##   yr_renovated = col_double(),
##   zipcode = col_double(),
##   lat = col_double(),
##   long = col_double(),
##   sqft_living15 = col_double(),
##   sqft_lot15 = col_double()
## )</code></pre>
<pre class="r"><code>mod &lt;- lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + yr_built, data = house)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ bedrooms + bathrooms + sqft_living + sqft_lot + 
##     yr_built, data = house)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1013022  -123183   -11192   101635  3057245 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.50e+06   3.55e+05   18.28  &lt; 2e-16 ***
## bedrooms    -7.75e+04   6.22e+03  -12.46  &lt; 2e-16 ***
## bathrooms    7.44e+04   9.96e+03    7.47  1.1e-13 ***
## sqft_living  3.19e+02   7.96e+00   40.05  &lt; 2e-16 ***
## sqft_lot    -8.86e-02   1.07e-01   -0.83     0.41    
## yr_built    -3.31e+03   1.83e+02  -18.09  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 253000 on 3159 degrees of freedom
## Multiple R-squared:  0.559,  Adjusted R-squared:  0.558 
## F-statistic:  801 on 5 and 3159 DF,  p-value: &lt;2e-16</code></pre>
<pre class="r"><code>anova(mod)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: price
##               Df   Sum Sq  Mean Sq F value Pr(&gt;F)    
## bedrooms       1 3.93e+13 3.93e+13  613.65 &lt;2e-16 ***
## bathrooms      1 8.76e+13 8.76e+13 1366.78 &lt;2e-16 ***
## sqft_living    1 1.09e+14 1.09e+14 1695.23 &lt;2e-16 ***
## sqft_lot       1 9.73e+10 9.73e+10    1.52   0.22    
## yr_built       1 2.10e+13 2.10e+13  327.11 &lt;2e-16 ***
## Residuals   3159 2.02e+14 6.41e+10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Here’s the corresponding output from Excel 2016 running under Parallels on an Apple laptop.</p>
<div class="figure">
<img src="https://s3-us-west-2.amazonaws.com/tuva/ExcelRegressionExample.png" alt="" />
<p class="caption">Excel Output</p>
</div>
<p>Let’s take the differences line-by-line</p>
<ol style="list-style-type: decimal">
<li>Summary Output: There may be an option tail to add it, but Excel doesn’t give you the formula for the model, the dependent and independent variables involved.</li>
<li>Excel analysis of variance (ANOVA), separately available in R is provided.</li>
<li>Regression Statistics:</li>
</ol>
<ul>
<li>Excel adds “multiple R,” which is the square root of R Squared. It’s a measure of the goodness of predicting prices from the model, which can be calculated in R, but is not normally given in the output of the table. I couldn’t find any way in Excel of specifying whether to use Kendall’s <em>tau</em> or Spearman’s <em>rho</em> statistic.</li>
<li>Excel’s remaining reports overlap those of R, except for the omission of detail of residuals and the F statistics.</li>
<li>Excel adds confidence intervals, which are available in R separately. It’s not clear what the difference between the pairs of column represents, and the 95% confidence interval is normally expessed in terms of a value for 92.5% and 97.5%.</li>
<li>Most of the results differ from those given my R for the same data. Except in the cases of the 0 valued p-levels for the <em>t</em> statistic, these are not large, and it is beyond the scope of my post here to figure out why. But apparently different computations are involved.</li>
</ul>
