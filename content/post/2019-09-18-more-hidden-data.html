---
title: "Missing Inaction--locating data holes"
author: "Richard Careaga"
date: '2019-09-18'
slug: data-holes
categories:
  - Data Science
tags: 
  - R
  - wrangling
  - matrix
---



<blockquote>
<p>Your hands can’t hit what your eyes can’t see. <em>Muhammad Ali</em></p>
</blockquote>
<p>In real-world data, there is often so much to be re-arranged, re-formatted and supplemented that it’s easy to overlook what <em>isn’t</em> there. <a href="https://technocrat.rbind.io/2019/08/24/hidden-missing-data/">I wrote about this</a> recently, but decided it could be improved.</p>
<div id="some-real-world-data-masked-and-anonymous" class="section level2">
<h2>Some real world data, masked and anonymous</h2>
<p>To illustrate some of the ways that a dataset can snooker you, I’ll be using a tibble with 22,471 rows and 38 columns. That’s 853,898 slots for values, far beyond eyeballing’s reach.</p>
<p>This observational data is to be used for a logistic model of VAR30 and VAR31 as the alternative dependent variables. The continuous variables are VAR1 (simply a record id), VAR2 and VAR7. All others are dichotomous, coded <code>1</code> for present and <code>0</code> for absent. VAR14-VAR29 are the principal independent variables of interest.</p>
<p>Here is the data layout.</p>
<pre><code>## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    22471 obs. of  38 variables:
##  $ VAR1 : int  420893 303936 191488 602555 319182 975171 327170 820841 385483 996417 ...
##  $ VAR2 : num  36 22 17 39 64 54 46 45 44 16 ...
##  $ VAR3 : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR4 : num  0 0 1 0 0 0 0 0 0 1 ...
##  $ VAR5 : num  1 1 0 1 1 1 1 1 1 0 ...
##  $ VAR6 : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR7 : num  2008 2008 2008 2008 2008 ...
##  $ VAR8 : num  1 0 0 0 0 0 0 0 0 0 ...
##  $ VAR9 : num  0 1 1 1 1 1 1 1 1 1 ...
##  $ VAR10: num  1 0 1 1 0 1 0 1 1 1 ...
##  $ VAR11: num  1 0 1 1 1 1 1 1 1 1 ...
##  $ VAR12: num  0 1 0 0 0 0 0 0 0 0 ...
##  $ VAR13: num  1 1 1 1 1 1 1 1 1 0 ...
##  $ VAR14: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR15: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR16: num  0 0 0 0 0 0 1 0 0 1 ...
##  $ VAR17: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR18: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR19: num  0 0 0 0 0 0 1 0 0 0 ...
##  $ VAR20: num  0 0 0 0 0 0 0 0 0 1 ...
##  $ VAR21: num  0 0 0 0 1 1 0 0 0 0 ...
##  $ VAR22: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR23: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR24: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR25: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR26: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR27: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR28: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR29: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR30: num  1 0 0 1 1 1 1 0 1 1 ...
##  $ VAR31: num  0 1 1 0 0 0 0 1 0 0 ...
##  $ VAR32: num  1 0 1 1 0 0 0 0 0 0 ...
##  $ VAR33: num  0 1 0 0 1 1 1 1 1 1 ...
##  $ VAR34: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR35: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR36: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR37: num  0 0 0 0 0 0 0 0 0 0 ...
##  $ VAR38: num  0 0 0 0 0 0 0 0 0 0 ...</code></pre>
</div>
<div id="whats-missing" class="section level1">
<h1>What’s missing?</h1>
<div id="the-not-applicables" class="section level2">
<h2>The not applicables</h2>
<p>To see if a data frame or tibble contains <em>any</em> missing values, the handy function is <code>anyNA()</code>. For our data, the result is TRUE.</p>
<p>A quick way to identify <em>how many</em> missing values there are is <code>CountCompCases{DescTools}.</code> (There is also <code>complete.cases{base}</code>, but it returns a logical list, rather than a summary.)</p>
<pre><code>## 
## Total rows:      22471
## Complete Cases:  22052 (98.1%)
## 
##     vname  nas  nas_p  cifnot  cifnot_p
## 1    VAR1    0   0.0%   22052     98.1%
## 2    VAR2    0   0.0%   22052     98.1%
## 3    VAR3    0   0.0%   22052     98.1%
## 4    VAR4    0   0.0%   22052     98.1%
## 5    VAR5    0   0.0%   22052     98.1%
## 6    VAR6    0   0.0%   22052     98.1%
## 7    VAR7    0   0.0%   22052     98.1%
## 8    VAR8    0   0.0%   22052     98.1%
## 9    VAR9    0   0.0%   22052     98.1%
## 10  VAR10    0   0.0%   22052     98.1%
## 11  VAR11    0   0.0%   22052     98.1%
## 12  VAR12    0   0.0%   22052     98.1%
## 13  VAR13    0   0.0%   22052     98.1%
## 14  VAR14    0   0.0%   22052     98.1%
## 15  VAR15    0   0.0%   22052     98.1%
## 16  VAR16    0   0.0%   22052     98.1%
## 17  VAR17    0   0.0%   22052     98.1%
## 18  VAR18    0   0.0%   22052     98.1%
## 19  VAR19    0   0.0%   22052     98.1%
## 20  VAR20    0   0.0%   22052     98.1%
## 21  VAR21    0   0.0%   22052     98.1%
## 22  VAR22    0   0.0%   22052     98.1%
## 23  VAR23    0   0.0%   22052     98.1%
## 24  VAR24    0   0.0%   22052     98.1%
## 25  VAR25    0   0.0%   22052     98.1%
## 26  VAR26    0   0.0%   22052     98.1%
## 27  VAR27    0   0.0%   22052     98.1%
## 28  VAR28    0   0.0%   22052     98.1%
## 29  VAR29    0   0.0%   22052     98.1%
## 30  VAR30    0   0.0%   22052     98.1%
## 31  VAR31    0   0.0%   22052     98.1%
## 32  VAR32  419   1.9%   22052     98.1%
## 33  VAR33  419   1.9%   22052     98.1%
## 34  VAR34  419   1.9%   22052     98.1%
## 35  VAR35  419   1.9%   22052     98.1%
## 36  VAR36  419   1.9%   22052     98.1%
## 37  VAR37  419   1.9%   22052     98.1%
## 38  VAR38    0   0.0%   22052     98.1%</code></pre>
<p>We see that 419 have <code>NA</code> values, about 1.9% of all observations.</p>
<div id="why-do-we-care" class="section level3">
<h3>Why do we care?</h3>
<p>NAs are squirrely when it comes to computation.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<ul>
<li>NA+1 = NA</li>
<li>NA-1 = NA</li>
<li>NA*1 = NA</li>
<li>NA/1 = NA</li>
<li>NA^1 = NA</li>
<li>NA^0 = 1</li>
<li>NA**2 = NA</li>
</ul>
<p>More likely than not, leaving them will throw errors.</p>
</div>
<div id="what-can-we-do" class="section level3">
<h3>What can we do?</h3>
<p>We have two alternatives: remove the rows with NAs or impute values. Because the NAs are only 1.9% of the data, I’m going to avoid all the skull sweat needed for good imputation (substituting some summary statistic for the NA). An additional consideration is that the variables with the NA are a bit dodgy to begin with for reasons not germane to the subject. They also turned out not to contribute to the modeling.</p>
</div>
<div id="how-do-we-remove-the-nas" class="section level3">
<h3>How do we remove the NAs?</h3>
<p>The <code>dplyr</code> <em>filter</em> function takes a logical argument and !isNA() provides one.</p>
<pre><code>mydata_no_na &lt;- mydata %&gt;% filter(!is.na(VAR32))</code></pre>
<p>A little sleight of hand was involved. I guessed that all of the variables with NA would be missing in each observation, so that by filtering on one, I’d hit them all. Sure enough, <code>anyNA()</code> confirmed my guess. If it hadn’t, of course, I would have needed to either run multiple filters or expand to</p>
<pre><code>mydata_no_na &lt;- mydata %&gt;% filter(!is.na(VAR32) | !is.na(VAR33) ...)</code></pre>
</div>
</div>
<div id="but-wait" class="section level2">
<h2>But wait!</h2>
<div id="dependent-variables-of-interest-are-all-zero-valued-for-some-observations" class="section level3">
<h3>Dependent variables of interest are all zero valued for some observations</h3>
<p>These data were assembled for a purpose – to determine the influence of a group of 16 variables (different types of medical trauma) on 2 variables considered separately (different types of internal injuries to the same organ). Observations in which none of the 16 variables were coded as present (1) shed no light on the problem.</p>
</div>
<div id="how-do-we-identify-them" class="section level3">
<h3>How do we identify them?</h3>
<p>Recall that each of the 16 variables is either valued as 1 or 0. If the sum of the variables for an observation is zero, we don’t need the observation for the main purposes of the model project. There are several possible ways to sum variables rowwise. I chose a little linear algebra jujitsu.</p>
<p><strong><em>Linear algebra!!!</em></strong> Relax, I don’t know much about this subject either. But I know a piece that’s simple and well suited to this problem.</p>
<p>Starting with <code>mydata_no_na</code>, the first step is to create an object for the 16 variables</p>
<pre><code>    interest_vars &lt;- c(&quot;VAR14&quot;, &quot;VAR15&quot;, &quot;VAR16&quot;, &quot;VAR17&quot;, &quot;VAR18&quot;, &quot;VAR19&quot;, &quot;VAR20&quot;, &quot;VAR21&quot;, &quot;VAR22&quot;, &quot;VAR23&quot;, &quot;VAR24&quot;, &quot;VAR25&quot;, &quot;VAR26&quot;, &quot;VAR27&quot;, &quot;VAR28&quot;, &quot;VAR29&quot;)</code></pre>
<p>and to use it to subset the NAs removed data</p>
<pre><code>    mydata_vars &lt;- mydata_no_na %&gt;% dplyr::select(interest_vars)</code></pre>
<p>Here comes the trick: It’s easy to convert a tibble into a matrix.</p>
<pre><code>    m &lt;- as.matrix(mydata_vars)</code></pre>
<p>Why do we want a matrix? A matrix is an object that you can perform row operations on easily.</p>
<pre><code>    rowtots &lt;- rowSums(m)</code></pre>
<p>We now have the sum of the 16 variables for each observation.</p>
<p>The return trip is equally easy.</p>
<pre><code>    vcode_count &lt;- enframe(rowtots) %&gt;% dplyr::select(value) %&gt;% dplyr::rename(V_COUNT = value)
    mydata_w_row_count &lt;- as_tibble(cbind(mydata_no_na, vcode_count))
    mydata_singlets &lt;- mydata_with_row_count %&gt;% filter(V_COUNT == 1)</code></pre>
<p>In short, we’ve added a row total field, <code>V_COUNT</code> and filtered for sums of 1.</p>
<p>The results are dramatic. We started with 22,471 observations, subtracted 419 NA, and then isolated the observations with one (only one) of the 16 variables and ended up with 9,114 observations.</p>
</div>
</div>
<div id="even-more" class="section level2">
<h2>Even more</h2>
<p>We’ve dealt with NA, eliminated observations with 0 or more than 1 occurrences. There is another possibility, observations with more than one variable of interest. These are easy to identify</p>
<pre><code>        mydata_multi &lt;- mydata_with_row_count %&gt;% filter(V_COUNT &gt; 1)</code></pre>
<p>and there are 2,954 observations. These are candidates for a separate model because there are potentially multiple interacting independent variables that influence the dependent variables. (Or they could represent inconsistencies in data collection.)</p>
</div>
</div>
<div id="flash-cards" class="section level1">
<h1>Flash cards</h1>
<ul>
<li>Don’t eyeball</li>
<li>Use anyNA()</li>
<li>Check for “useless” observations</li>
<li>Check for observations needing different treatment</li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“Numerical computations using NA will normally result in NA: a possible exception is where NaN is also involved, in which case either might result (which may depend on the R platform). Logical computations treat NA as a missing TRUE/FALSE value, and so may return TRUE or FALSE if the expression does not depend on the NA operand.” <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html">[NA]{base}</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
